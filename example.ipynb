{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4baf085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6f3c19",
   "metadata": {},
   "source": [
    "This is the max norm constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "622bb872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD with MaxNorm constraint\n",
    "# max_norm must be packaged inside parameters\n",
    "class MNSGD(torch.optim.SGD):    \n",
    "    def step(self):\n",
    "        super().step()\n",
    "        with torch.no_grad():\n",
    "            # apply the max_norm weight correction per group of parameters\n",
    "            for group in self.param_groups:\n",
    "                # rescale iff group has specified max_norm\n",
    "                if group.get('max_norm'):\n",
    "                    for tensor in group['params']:\n",
    "                        if tensor.dim() > 1:\n",
    "                            torch.renorm(tensor, p=2, dim=0, maxnorm=group['max_norm'], out=tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1583404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "569a28d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e12b0f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, dropout_p=0.3):\n",
    "        # call __init__ from the parent class: nn.Module.\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        # this flattens the images in the batch to 1d tensors suitable for nn.Sequential\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(128, 80),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Linear(80, 10)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.flatten(x)\n",
    "        x = self.hidden(x)\n",
    "        logits = self.output(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eb599b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    model.train()                  # enter training mode to activate dropout\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        X, y = batch[0].to(device), batch[1].to(device)\n",
    "        pred = model(X)            #*mat: evaluate the model batch_size times and put results in a tensor \"next\" to each other\n",
    "        loss = loss_fn(pred, y)    #*mat: compute the loss function\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()      # place in memory for the derivative needs resetting\n",
    "        loss.backward()            #*mat: calculate the derivative (**)\n",
    "        optimizer.step()           # call the black box algoritm that do the magic\n",
    "        \n",
    "        # print the progress every 100th batch.\n",
    "        if i % 100 == 0:\n",
    "            loss, current = loss.item(), i*len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf073520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    model.eval()                   # enter testing mode to prevent dropout\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X, y = batch[0].to(device), batch[1].to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    print(f\"Test error: \\n Accuracy {(100*correct):>0.1f}%, Avg loss {test_loss:>8f} \\n \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5238480e",
   "metadata": {},
   "source": [
    "Besides training and testing loops, I've also added a `check_norms()` to verify that the max norm constraint is implemented correctly, up to a tolerance (0.0001 by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcca5ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_norms(model, max_norm, tol=1e-4):\n",
    "    with torch.no_grad():\n",
    "        norm_status = 'Norms OK'\n",
    "        max_vector_norm = 0\n",
    "        for i, tensor in enumerate(model.hidden.parameters()):\n",
    "            if tensor.dim() > 1:\n",
    "                vector_norm = torch.max(\n",
    "                    torch.linalg.vector_norm(tensor, ord=2, \n",
    "                                             dim=1, keepdim=True)\n",
    "                )\n",
    "                if vector_norm - tol > max_norm:\n",
    "                    norm_status = 'Norms NOK'\n",
    "                if vector_norm > max_vector_norm:\n",
    "                    max_vector_norm = vector_norm\n",
    "        print(norm_status + f\", max vector norm is {max_vector_norm}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815a033d",
   "metadata": {},
   "source": [
    "This is where max norm is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "017e8009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "learning_rate = 0.5\n",
    "batch_size = 100\n",
    "epochs = 10\n",
    "max_norm = 2\n",
    "momentum = 0.5\n",
    "\n",
    "# define model\n",
    "model = NeuralNetwork(dropout_p = 0.5).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = NeuralNetwork().to(device)\n",
    "optimizer = MNSGD(\n",
    "    [\n",
    "        {'params': model.hidden.parameters(), 'max_norm': max_norm }, \n",
    "        {'params': model.output.parameters() }\n",
    "    ],\n",
    "    lr=learning_rate,\n",
    "    momentum=momentum\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2213a1",
   "metadata": {},
   "source": [
    "This is a train/test routine that includes a check of whether weight vectors satisfy the max norm constraint after every test loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becccbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "----------------------------\n",
      "loss: 2.308669 [    0/60000]\n",
      "loss: 0.436984 [10000/60000]\n",
      "loss: 0.492524 [20000/60000]\n",
      "loss: 0.334368 [30000/60000]\n",
      "loss: 0.372025 [40000/60000]\n",
      "loss: 0.467952 [50000/60000]\n",
      "Test error: \n",
      " Accuracy 93.6%, Avg loss 0.200647 \n",
      " \n",
      "Norms OK, max vector norm is 2.000000238418579\n",
      "\n",
      "Epoch 2\n",
      "----------------------------\n",
      "loss: 0.253347 [    0/60000]\n",
      "loss: 0.236187 [10000/60000]\n",
      "loss: 0.343207 [20000/60000]\n",
      "loss: 0.372917 [30000/60000]\n",
      "loss: 0.235315 [40000/60000]\n",
      "loss: 0.212042 [50000/60000]\n",
      "Test error: \n",
      " Accuracy 95.7%, Avg loss 0.146674 \n",
      " \n",
      "Norms OK, max vector norm is 2.000000238418579\n",
      "\n",
      "Epoch 3\n",
      "----------------------------\n",
      "loss: 0.258380 [    0/60000]\n",
      "loss: 0.123156 [10000/60000]\n",
      "loss: 0.245546 [20000/60000]\n",
      "loss: 0.337066 [30000/60000]\n",
      "loss: 0.414379 [40000/60000]\n",
      "loss: 0.301445 [50000/60000]\n",
      "Test error: \n",
      " Accuracy 95.9%, Avg loss 0.138769 \n",
      " \n",
      "Norms OK, max vector norm is 2.000000238418579\n",
      "\n",
      "Epoch 4\n",
      "----------------------------\n",
      "loss: 0.483095 [    0/60000]\n",
      "loss: 0.209205 [10000/60000]\n",
      "loss: 0.277883 [20000/60000]\n",
      "loss: 0.208181 [30000/60000]\n",
      "loss: 0.151962 [40000/60000]\n",
      "loss: 0.272658 [50000/60000]\n",
      "Test error: \n",
      " Accuracy 95.8%, Avg loss 0.139551 \n",
      " \n",
      "Norms OK, max vector norm is 2.000000238418579\n",
      "\n",
      "Epoch 5\n",
      "----------------------------\n",
      "loss: 0.174831 [    0/60000]\n",
      "loss: 0.335367 [10000/60000]\n",
      "loss: 0.262890 [20000/60000]\n",
      "loss: 0.109417 [30000/60000]\n",
      "loss: 0.250971 [40000/60000]\n",
      "loss: 0.250245 [50000/60000]\n",
      "Test error: \n",
      " Accuracy 96.0%, Avg loss 0.121393 \n",
      " \n",
      "Norms OK, max vector norm is 2.000000238418579\n",
      "\n",
      "Epoch 6\n",
      "----------------------------\n",
      "loss: 0.226746 [    0/60000]\n",
      "loss: 0.210122 [10000/60000]\n",
      "loss: 0.188203 [20000/60000]\n",
      "loss: 0.233506 [30000/60000]\n",
      "loss: 0.269764 [40000/60000]\n",
      "loss: 0.450293 [50000/60000]\n",
      "Test error: \n",
      " Accuracy 96.2%, Avg loss 0.125947 \n",
      " \n",
      "Norms OK, max vector norm is 2.000000238418579\n",
      "\n",
      "Epoch 7\n",
      "----------------------------\n",
      "loss: 0.414050 [    0/60000]\n",
      "loss: 0.232335 [10000/60000]\n",
      "loss: 0.202112 [20000/60000]\n",
      "loss: 0.334821 [30000/60000]\n",
      "loss: 0.150339 [40000/60000]\n",
      "loss: 0.273351 [50000/60000]\n",
      "Test error: \n",
      " Accuracy 96.5%, Avg loss 0.113849 \n",
      " \n",
      "Norms OK, max vector norm is 2.000000238418579\n",
      "\n",
      "Epoch 8\n",
      "----------------------------\n",
      "loss: 0.120149 [    0/60000]\n",
      "loss: 0.180171 [10000/60000]\n",
      "loss: 0.375802 [20000/60000]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=100, shuffle=True, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size = 100, shuffle=True, pin_memory=True)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n----------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "    check_norms(model, max_norm)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22763ff",
   "metadata": {},
   "source": [
    "The norms are correctly scaled."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
